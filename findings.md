# Findings

## Discovery Answers
- **North Star:** Local LLM Testcase generator based on User input with a proper Template using Ollama (llama3.2).
- **Integrations:** Ollama (Local).
- **Source of Truth:** NA (Direct User Input).
- **Delivery Payload:** A UI chat where users enter input and receive generated testcases.
- **Behavioral Rules:** Input-to-Output generation using Local LLM in Ollama.

## Research
- Goal: Create a local LLM Testcase generator using Ollama.
- Protocol: B.L.A.S.T.
- Model: `llama3.2`.

## Constraints
- Must follow BLAST protocol.
- Local only.
